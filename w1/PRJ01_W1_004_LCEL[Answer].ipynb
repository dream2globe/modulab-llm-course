{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7cd25bfb",
      "metadata": {},
      "source": [
        "# LangChain LCEL\n",
        "\n",
        "### **학습 목표:** LangChain의 LCEL을 사용하여 LLM 체인을 구성한다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2217fa83",
      "metadata": {},
      "source": [
        "## LCEL(LangChain Expression Language) \n",
        "\n",
        "- **LCEL**은 `|` 연산자를 사용해 컴포넌트들을 순차적으로 연결하는 선언적 체이닝을 지원합니다\n",
        "\n",
        "- **재사용성**이 높아 정의된 체인을 다른 체인의 컴포넌트로 활용할 수 있습니다\n",
        "\n",
        "- **다양한 실행 방식**(.invoke(), .batch(), .stream(), .astream())으로 동기/비동기 처리가 가능합니다\n",
        "\n",
        "- **배치 처리**시 자동 최적화를 통해 효율적인 작업 수행이 가능합니다\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bfd9328",
      "metadata": {
        "id": "8bfd9328"
      },
      "source": [
        "### 1. 환경 설정\n",
        "\n",
        "- langsmith 가입 필요 (https://www.langchain.com/langsmith)\n",
        "- 부분 무료"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "15ac3ddb",
      "metadata": {
        "id": "15ac3ddb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "false\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Langsmith tracing 여부를 확인 (true: langsmith 추척 활성화, false: langsmith 추척 비활성화)\n",
        "import os\n",
        "print(os.getenv('LANGCHAIN_TRACING_V2'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "129b8eee",
      "metadata": {
        "id": "129b8eee"
      },
      "source": [
        "### 2. LCEL  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68074eb8",
      "metadata": {
        "id": "68074eb8"
      },
      "source": [
        "#### 1) **Prompt + LLM**\n",
        "\n",
        "* 기본 구조: `Prompt | LLM` 형태로, 파이프(|) 연산자를 사용해 프롬프트와 LLM을 순차적으로 연결합니다.\n",
        "\n",
        "* 데이터 흐름: 사용자 입력이 Prompt 템플릿을 통해 처리된 후, LLM에 전달되어 최종 응답이 생성됩니다.\n",
        "\n",
        "* 실행 순서: 파이프라인은 왼쪽에서 오른쪽으로 순차적으로 실행되며, 각 컴포넌트의 출력이 다음 컴포넌트의 입력으로 전달됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3722db67",
      "metadata": {
        "id": "3722db67"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# LLM model\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\", \n",
        "    temperature=0.3, \n",
        "    max_tokens=100,\n",
        "    )\n",
        "\n",
        "# 모델에 프롬프트를 입력\n",
        "response = llm.invoke(\"탄소의 원자 번호는 무엇인가요?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9e87eef4",
      "metadata": {
        "id": "9e87eef4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='탄소의 원자 번호는 6입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 18, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-964adab7-8467-446e-b247-7aaf272cbb12-0', usage_metadata={'input_tokens': 18, 'output_tokens': 12, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 응답 객체 확인\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "47ce51f7",
      "metadata": {
        "id": "47ce51f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'탄소의 원자 번호는 6입니다.'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 응답 객체의 텍스트를 확인\n",
        "response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "552cd7af",
      "metadata": {
        "id": "552cd7af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'token_usage': {'completion_tokens': 12,\n",
              "  'prompt_tokens': 18,\n",
              "  'total_tokens': 30,\n",
              "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
              "   'audio_tokens': 0,\n",
              "   'reasoning_tokens': 0,\n",
              "   'rejected_prediction_tokens': 0},\n",
              "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
              " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
              " 'system_fingerprint': 'fp_72ed7ab54c',\n",
              " 'finish_reason': 'stop',\n",
              " 'logprobs': None}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 응답 객체의 메타데이터를 확인\n",
        "response.response_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0113cf90",
      "metadata": {
        "id": "0113cf90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "템플릿 변수:\n",
            "- 필수 변수: ['question', 'topic']\n",
            "\n",
            "생성된 프롬프트:\n",
            "\n",
            "당신은 인공지능 분야의 전문가입니다. 인공지능에 관한 다음 질문에 답변해주세요.\n",
            "질문: 딥러닝과 머신러닝의 주요 차이점은 무엇인가요?\n",
            "답변: \n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# 템플릿 문자열 정의\n",
        "template = \"\"\"\n",
        "당신은 {topic} 분야의 전문가입니다. {topic}에 관한 다음 질문에 답변해주세요.\n",
        "질문: {question}\n",
        "답변: \"\"\"\n",
        "\n",
        "# PromptTemplate 객체 생성\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# 템플릿 사용 예시\n",
        "formatted_prompt = prompt.format(\n",
        "    topic=\"인공지능\",\n",
        "    question=\"딥러닝과 머신러닝의 주요 차이점은 무엇인가요?\"\n",
        ")\n",
        "\n",
        "print(\"템플릿 변수:\")\n",
        "print(f\"- 필수 변수: {prompt.input_variables}\")\n",
        "print(\"\\n생성된 프롬프트:\")\n",
        "print(formatted_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "25b245d7",
      "metadata": {
        "id": "25b245d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "당신은 {topic} 분야의 전문가입니다. {topic}에 관한 다음 질문에 답변해주세요.\n",
            "질문: {question}\n",
            "답변: \n"
          ]
        }
      ],
      "source": [
        "# prompt 객체의 템플릿을 확인\n",
        "print(prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c801f727",
      "metadata": {
        "id": "c801f727"
      },
      "outputs": [],
      "source": [
        "# chain을 구성\n",
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b53e9def",
      "metadata": {
        "id": "b53e9def"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['question', 'topic'], input_types={}, partial_variables={}, template='\\n당신은 {topic} 분야의 전문가입니다. {topic}에 관한 다음 질문에 답변해주세요.\\n질문: {question}\\n답변: ')\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1319c9c10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1319cb7a0>, root_client=<openai.OpenAI object at 0x1312cb9b0>, root_async_client=<openai.AsyncOpenAI object at 0x1319c9c70>, model_name='gpt-4o-mini', temperature=0.3, model_kwargs={}, openai_api_key=SecretStr('**********'), max_tokens=100)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# chain 객체 확인\n",
        "chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6d4f916f",
      "metadata": {
        "id": "6d4f916f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'properties': {'question': {'title': 'Question', 'type': 'string'},\n",
              "  'topic': {'title': 'Topic', 'type': 'string'}},\n",
              " 'required': ['question', 'topic'],\n",
              " 'title': 'PromptInput',\n",
              " 'type': 'object'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# chain 객체의 입력 스키마를 확인\n",
        "chain.input_schema.model_json_schema() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6015e000",
      "metadata": {
        "id": "6015e000"
      },
      "outputs": [],
      "source": [
        "# chain 실행\n",
        "response = chain.invoke( \n",
        "    {\n",
        "        \"topic\": \"화학(Chemistry)\", \n",
        "        \"question\": \"탄소의 원자 번호는 무엇인가요?\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fa75f9ee",
      "metadata": {
        "id": "fa75f9ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='탄소의 원자 번호는 6입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 54, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-88bb1c86-2505-4d31-a3c5-cabab85d1e0c-0', usage_metadata={'input_tokens': 54, 'output_tokens': 12, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 응답 객체를 출력\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fb0a3843",
      "metadata": {
        "id": "fb0a3843"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'탄소의 원자 번호는 6입니다.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 응답 객체의 텍스트를 출력\n",
        "response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c6c5710c",
      "metadata": {
        "id": "c6c5710c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'token_usage': {'completion_tokens': 12,\n",
              "  'prompt_tokens': 54,\n",
              "  'total_tokens': 66,\n",
              "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
              "   'audio_tokens': 0,\n",
              "   'reasoning_tokens': 0,\n",
              "   'rejected_prediction_tokens': 0},\n",
              "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
              " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
              " 'system_fingerprint': 'fp_72ed7ab54c',\n",
              " 'finish_reason': 'stop',\n",
              " 'logprobs': None}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 응답 객체의 메타데이터를 출력\n",
        "response.response_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "983d4157",
      "metadata": {
        "id": "983d4157"
      },
      "source": [
        "#### **2) Prompt + LLM + Output Parser**\n",
        "\n",
        "* 데이터 파이프라인: `Prompt | LLM | OutputParser` 형태로 구성되어 LLM의 출력을 구조화된 형식으로 변환합니다.\n",
        "\n",
        "* Parser 종류: JSON, XML 등 다양한 형식의 파서를 지원하여 LLM 출력을 원하는 데이터 구조로 변환할 수 있습니다.\n",
        "\n",
        "* 유효성 검증: Parser가 출력 형식을 검증하여 잘못된 형식의 응답을 필터링하고 안정적인 데이터 처리를 보장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7e6b7607",
      "metadata": {
        "id": "7e6b7607"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'탄소의 원자 번호는 6입니다.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### 문자열 출력 파서 \n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 출력 파서를 생성\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 출력 파서를 실행\n",
        "output_parser.invoke(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "158608d9",
      "metadata": {
        "id": "158608d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'탄소의 원자 번호는 6입니다.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 출력 파서를 chain에 추가\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "# chain을 실행\n",
        "response = chain.invoke(\n",
        "    {\n",
        "        \"topic\": \"화학(Chemistry)\", \n",
        "        \"question\": \"탄소의 원자 번호는 무엇인가요?\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# 응답 객체를 출력\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1e95ba99",
      "metadata": {
        "id": "1e95ba99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'properties': {'question': {'title': 'Question', 'type': 'string'},\n",
              "  'topic': {'title': 'Topic', 'type': 'string'}},\n",
              " 'required': ['question', 'topic'],\n",
              " 'title': 'PromptInput',\n",
              " 'type': 'object'}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# input_schema (chain)\n",
        "chain.input_schema.model_json_schema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ba0d6997",
      "metadata": {
        "id": "ba0d6997"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'properties': {'question': {'title': 'Question', 'type': 'string'},\n",
              "  'topic': {'title': 'Topic', 'type': 'string'}},\n",
              " 'required': ['question', 'topic'],\n",
              " 'title': 'PromptInput',\n",
              " 'type': 'object'}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# input_schema (prompt)\n",
        "prompt.input_schema.model_json_schema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7abd42cd",
      "metadata": {
        "id": "7abd42cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'$defs': {'AIMessage': {'additionalProperties': True,\n",
              "   'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
              "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
              "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
              "       'type': 'array'}],\n",
              "     'title': 'Content'},\n",
              "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
              "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
              "    'type': {'const': 'ai',\n",
              "     'default': 'ai',\n",
              "     'title': 'Type',\n",
              "     'type': 'string'},\n",
              "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Name'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Id'},\n",
              "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
              "    'tool_calls': {'default': [],\n",
              "     'items': {'$ref': '#/$defs/ToolCall'},\n",
              "     'title': 'Tool Calls',\n",
              "     'type': 'array'},\n",
              "    'invalid_tool_calls': {'default': [],\n",
              "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
              "     'title': 'Invalid Tool Calls',\n",
              "     'type': 'array'},\n",
              "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
              "      {'type': 'null'}],\n",
              "     'default': None}},\n",
              "   'required': ['content'],\n",
              "   'title': 'AIMessage',\n",
              "   'type': 'object'},\n",
              "  'AIMessageChunk': {'additionalProperties': True,\n",
              "   'description': 'Message chunk from an AI.',\n",
              "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
              "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
              "       'type': 'array'}],\n",
              "     'title': 'Content'},\n",
              "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
              "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
              "    'type': {'const': 'AIMessageChunk',\n",
              "     'default': 'AIMessageChunk',\n",
              "     'title': 'Type',\n",
              "     'type': 'string'},\n",
              "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Name'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Id'},\n",
              "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
              "    'tool_calls': {'default': [],\n",
              "     'items': {'$ref': '#/$defs/ToolCall'},\n",
              "     'title': 'Tool Calls',\n",
              "     'type': 'array'},\n",
              "    'invalid_tool_calls': {'default': [],\n",
              "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
              "     'title': 'Invalid Tool Calls',\n",
              "     'type': 'array'},\n",
              "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
              "      {'type': 'null'}],\n",
              "     'default': None},\n",
              "    'tool_call_chunks': {'default': [],\n",
              "     'items': {'$ref': '#/$defs/ToolCallChunk'},\n",
              "     'title': 'Tool Call Chunks',\n",
              "     'type': 'array'}},\n",
              "   'required': ['content'],\n",
              "   'title': 'AIMessageChunk',\n",
              "   'type': 'object'},\n",
              "  'ChatMessage': {'additionalProperties': True,\n",
              "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
              "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
              "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
              "       'type': 'array'}],\n",
              "     'title': 'Content'},\n",
              "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
              "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
              "    'type': {'const': 'chat',\n",
              "     'default': 'chat',\n",
              "     'title': 'Type',\n",
              "     'type': 'string'},\n",
              "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Name'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Id'},\n",
              "    'role': {'title': 'Role', 'type': 'string'}},\n",
              "   'required': ['content', 'role'],\n",
              "   'title': 'ChatMessage',\n",
              "   'type': 'object'},\n",
              "  'ChatMessageChunk': {'additionalProperties': True,\n",
              "   'description': 'Chat Message chunk.',\n",
              "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
              "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
              "       'type': 'array'}],\n",
              "     'title': 'Content'},\n",
              "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
              "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
              "    'type': {'const': 'ChatMessageChunk',\n",
              "     'default': 'ChatMessageChunk',\n",
              "     'title': 'Type',\n",
              "     'type': 'string'},\n",
              "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Name'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Id'},\n",
              "    'role': {'title': 'Role', 'type': 'string'}},\n",
              "   'required': ['content', 'role'],\n",
              "   'title': 'ChatMessageChunk',\n",
              "   'type': 'object'},\n",
              "  'ChatPromptValueConcrete': {'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.',\n",
              "   'properties': {'messages': {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'},\n",
              "       {'$ref': '#/$defs/HumanMessage'},\n",
              "       {'$ref': '#/$defs/ChatMessage'},\n",
              "       {'$ref': '#/$defs/SystemMessage'},\n",
              "       {'$ref': '#/$defs/FunctionMessage'},\n",
              "       {'$ref': '#/$defs/ToolMessage'},\n",
              "       {'$ref': '#/$defs/AIMessageChunk'},\n",
              "       {'$ref': '#/$defs/HumanMessageChunk'},\n",
              "       {'$ref': '#/$defs/ChatMessageChunk'},\n",
              "       {'$ref': '#/$defs/SystemMessageChunk'},\n",
              "       {'$ref': '#/$defs/FunctionMessageChunk'},\n",
              "       {'$ref': '#/$defs/ToolMessageChunk'}]},\n",
              "     'title': 'Messages',\n",
              "     'type': 'array'},\n",
              "    'type': {'const': 'ChatPromptValueConcrete',\n",
              "     'default': 'ChatPromptValueConcrete',\n",
              "     'title': 'Type',\n",
              "     'type': 'string'}},\n",
              "   'required': ['messages'],\n",
              "   'title': 'ChatPromptValueConcrete',\n",
              "   'type': 'object'},\n",
              "  'FunctionMessage': {'additionalProperties': True,\n",
              "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
              "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
              "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
              "       'type': 'array'}],\n",
              "     'title': 'Content'},\n",
              "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
              "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
              "    'type': {'const': 'function',\n",
              "     'default': 'function',\n",
              "     'title': 'Type',\n",
              "     'type': 'string'},\n",
              "    'name': {'title': 'Name', 'type': 'string'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Id'}},\n",
              "   'required': ['content', 'name'],\n",
              "   'title': 'FunctionMessage',\n",
              "   'type': 'object'},\n",
              "  'FunctionMessageChunk': {'additionalProperties': True,\n",
              "   'description': 'Function Message chunk.',\n",
              "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
              "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
              "       'type': 'array'}],\n",
              "     'title': 'Content'},\n",
              "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
              "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
              "    'type': {'const': 'FunctionMessageChunk',\n",
              "     'default': 'FunctionMessageChunk',\n",
              "     'title': 'Type',\n",
              "     'type': 'string'},\n",
              "    'name': {'title': 'Name', 'type': 'string'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Id'}},\n",
              "   'required': ['content', 'name'],\n",
              "   'title': 'FunctionMessageChunk',\n",
              "   'type': 'object'},\n",
              "  'HumanMessage': {'additionalProperties': True,\n",
              "   'description': 'Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))',\n",
              "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
              "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
              "       'type': 'array'}],\n",
              "     'title': 'Content'},\n",
              "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
              "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
              "    'type': {'const': 'human',\n",
              "     'default': 'human',\n",
              "     'title': 'Type',\n",
              "     'type': 'string'},\n",
              "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Name'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Id'},\n",
              "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
              "   'required': ['content'],\n",
              "   'title': 'HumanMessage',\n",
              "   'type': 'object'},\n",
              "  'HumanMessageChunk': {'additionalProperties': True,\n",
              "   'description': 'Human Message chunk.',\n",
              "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
              "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
              "       'type': 'array'}],\n",
              "     'title': 'Content'},\n",
              "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
              "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
              "    'type': {'const': 'HumanMessageChunk',\n",
              "     'default': 'HumanMessageChunk',\n",
              "     'title': 'Type',\n",
              "     'type': 'string'},\n",
              "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Name'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Id'},\n",
              "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
              "   'required': ['content'],\n",
              "   'title': 'HumanMessageChunk',\n",
              "   'type': 'object'},\n",
              "  'InputTokenDetails': {'description': 'Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"cache_creation\": 200,\\n            \"cache_read\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
              "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
              "    'cache_creation': {'title': 'Cache Creation', 'type': 'integer'},\n",
              "    'cache_read': {'title': 'Cache Read', 'type': 'integer'}},\n",
              "   'title': 'InputTokenDetails',\n",
              "   'type': 'object'},\n",
              "  'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)',\n",
              "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'title': 'Name'},\n",
              "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
              "    'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'title': 'Error'},\n",
              "    'type': {'const': 'invalid_tool_call', 'title': 'Type', 'type': 'string'}},\n",
              "   'required': ['name', 'args', 'id', 'error'],\n",
              "   'title': 'InvalidToolCall',\n",
              "   'type': 'object'},\n",
              "  'OutputTokenDetails': {'description': 'Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"reasoning\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
              "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
              "    'reasoning': {'title': 'Reasoning', 'type': 'integer'}},\n",
              "   'title': 'OutputTokenDetails',\n",
              "   'type': 'object'},\n",
              "  'StringPromptValue': {'description': 'String prompt value.',\n",
              "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
              "    'type': {'const': 'StringPromptValue',\n",
              "     'default': 'StringPromptValue',\n",
              "     'title': 'Type',\n",
              "     'type': 'string'}},\n",
              "   'required': ['text'],\n",
              "   'title': 'StringPromptValue',\n",
              "   'type': 'object'},\n",
              "  'SystemMessage': {'additionalProperties': True,\n",
              "   'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))',\n",
              "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
              "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
              "       'type': 'array'}],\n",
              "     'title': 'Content'},\n",
              "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
              "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
              "    'type': {'const': 'system',\n",
              "     'default': 'system',\n",
              "     'title': 'Type',\n",
              "     'type': 'string'},\n",
              "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Name'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Id'}},\n",
              "   'required': ['content'],\n",
              "   'title': 'SystemMessage',\n",
              "   'type': 'object'},\n",
              "  'SystemMessageChunk': {'additionalProperties': True,\n",
              "   'description': 'System Message chunk.',\n",
              "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
              "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
              "       'type': 'array'}],\n",
              "     'title': 'Content'},\n",
              "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
              "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
              "    'type': {'const': 'SystemMessageChunk',\n",
              "     'default': 'SystemMessageChunk',\n",
              "     'title': 'Type',\n",
              "     'type': 'string'},\n",
              "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Name'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Id'}},\n",
              "   'required': ['content'],\n",
              "   'title': 'SystemMessageChunk',\n",
              "   'type': 'object'},\n",
              "  'ToolCall': {'description': 'Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"name\": \"foo\",\\n            \"args\": {\"a\": 1},\\n            \"id\": \"123\"\\n        }\\n\\n    This represents a request to call the tool named \"foo\" with arguments {\"a\": 1}\\n    and an identifier of \"123\".',\n",
              "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
              "    'args': {'title': 'Args', 'type': 'object'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
              "    'type': {'const': 'tool_call', 'title': 'Type', 'type': 'string'}},\n",
              "   'required': ['name', 'args', 'id'],\n",
              "   'title': 'ToolCall',\n",
              "   'type': 'object'},\n",
              "  'ToolCallChunk': {'description': 'A chunk of a tool call (e.g., as part of a stream).\\n\\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    left_chunks = [ToolCallChunk(name=\"foo\", args=\\'{\"a\":\\', index=0)]\\n    right_chunks = [ToolCallChunk(name=None, args=\\'1}\\', index=0)]\\n\\n    (\\n        AIMessageChunk(content=\"\", tool_call_chunks=left_chunks)\\n        + AIMessageChunk(content=\"\", tool_call_chunks=right_chunks)\\n    ).tool_call_chunks == [ToolCallChunk(name=\\'foo\\', args=\\'{\"a\":1}\\', index=0)]',\n",
              "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'title': 'Name'},\n",
              "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
              "    'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
              "     'title': 'Index'},\n",
              "    'type': {'const': 'tool_call_chunk', 'title': 'Type', 'type': 'string'}},\n",
              "   'required': ['name', 'args', 'id', 'index'],\n",
              "   'title': 'ToolCallChunk',\n",
              "   'type': 'object'},\n",
              "  'ToolMessage': {'additionalProperties': True,\n",
              "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\\'42\\', tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \"stdout\": \"From the graph we can see that the correlation between x and y is ...\",\\n            \"stderr\": None,\\n            \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\"stdout\"],\\n            artifact=tool_output,\\n            tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
              "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
              "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
              "       'type': 'array'}],\n",
              "     'title': 'Content'},\n",
              "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
              "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
              "    'type': {'const': 'tool',\n",
              "     'default': 'tool',\n",
              "     'title': 'Type',\n",
              "     'type': 'string'},\n",
              "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Name'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Id'},\n",
              "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
              "    'artifact': {'default': None, 'title': 'Artifact'},\n",
              "    'status': {'default': 'success',\n",
              "     'enum': ['success', 'error'],\n",
              "     'title': 'Status',\n",
              "     'type': 'string'}},\n",
              "   'required': ['content', 'tool_call_id'],\n",
              "   'title': 'ToolMessage',\n",
              "   'type': 'object'},\n",
              "  'ToolMessageChunk': {'additionalProperties': True,\n",
              "   'description': 'Tool Message chunk.',\n",
              "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
              "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
              "       'type': 'array'}],\n",
              "     'title': 'Content'},\n",
              "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
              "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
              "    'type': {'const': 'ToolMessageChunk',\n",
              "     'default': 'ToolMessageChunk',\n",
              "     'title': 'Type',\n",
              "     'type': 'string'},\n",
              "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Name'},\n",
              "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
              "     'default': None,\n",
              "     'title': 'Id'},\n",
              "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
              "    'artifact': {'default': None, 'title': 'Artifact'},\n",
              "    'status': {'default': 'success',\n",
              "     'enum': ['success', 'error'],\n",
              "     'title': 'Status',\n",
              "     'type': 'string'}},\n",
              "   'required': ['content', 'tool_call_id'],\n",
              "   'title': 'ToolMessageChunk',\n",
              "   'type': 'object'},\n",
              "  'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"input_tokens\": 350,\\n            \"output_tokens\": 240,\\n            \"total_tokens\": 590,\\n            \"input_token_details\": {\\n                \"audio\": 10,\\n                \"cache_creation\": 200,\\n                \"cache_read\": 100,\\n            },\\n            \"output_token_details\": {\\n                \"audio\": 10,\\n                \"reasoning\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.',\n",
              "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
              "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
              "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'},\n",
              "    'input_token_details': {'$ref': '#/$defs/InputTokenDetails'},\n",
              "    'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}},\n",
              "   'required': ['input_tokens', 'output_tokens', 'total_tokens'],\n",
              "   'title': 'UsageMetadata',\n",
              "   'type': 'object'}},\n",
              " 'anyOf': [{'type': 'string'},\n",
              "  {'$ref': '#/$defs/StringPromptValue'},\n",
              "  {'$ref': '#/$defs/ChatPromptValueConcrete'},\n",
              "  {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'},\n",
              "     {'$ref': '#/$defs/HumanMessage'},\n",
              "     {'$ref': '#/$defs/ChatMessage'},\n",
              "     {'$ref': '#/$defs/SystemMessage'},\n",
              "     {'$ref': '#/$defs/FunctionMessage'},\n",
              "     {'$ref': '#/$defs/ToolMessage'},\n",
              "     {'$ref': '#/$defs/AIMessageChunk'},\n",
              "     {'$ref': '#/$defs/HumanMessageChunk'},\n",
              "     {'$ref': '#/$defs/ChatMessageChunk'},\n",
              "     {'$ref': '#/$defs/SystemMessageChunk'},\n",
              "     {'$ref': '#/$defs/FunctionMessageChunk'},\n",
              "     {'$ref': '#/$defs/ToolMessageChunk'}]},\n",
              "   'type': 'array'}],\n",
              " 'title': 'ChatOpenAIInput'}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# input_schema (model)\n",
        "llm.input_schema.model_json_schema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4456bde9",
      "metadata": {
        "id": "4456bde9"
      },
      "source": [
        "### 3. Runnable\n",
        "\n",
        "* 실행 인터페이스: 모든 LangChain 컴포넌트는 Runnable 인터페이스를 구현하여 일관된 방식으로 실행됩니다.\n",
        "\n",
        "* 실행 메서드: `.invoke()`, `.batch()`, `.stream()`, `.astream()` 등 다양한 실행 방식을 제공합니다.\n",
        "\n",
        "* 호환성: 모든 Runnable 컴포넌트는 파이프(|) 연산자를 통해 연결 가능하며, 재사용이 용이합니다.\n",
        "\n",
        "* Runnable의 주요 유형:\n",
        "\n",
        "    * `RunnableSequence`: 여러 Runnable을 순차적으로 실행\n",
        "    * `RunnablePassthrough`: 입력을 그대로 다음 단계로 전달    \n",
        "    * `RunnableParallel`: 여러 Runnable을 병렬로 실행\n",
        "    * `RunnableLambda`: 파이썬 함수를 Runnable로 래핑하여 체인에서 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "022d0cfd",
      "metadata": {},
      "source": [
        "#### 1) **RunnableSequence**\n",
        "\n",
        "- **RunnableSequence**는 컴포넌트들을 연결하여 순차적으로 데이터를 처리하는 체인입니다\n",
        "\n",
        "- `|` 연산자로 연결된 각 단계의 **출력이 다음 단계의 입력**으로 전달됩니다\n",
        "\n",
        "- **다양한 실행 방식**(동기/비동기, 배치/스트리밍)을 지원합니다\n",
        "\n",
        "- LLM 체인, 데이터 파이프라인, 자동화된 작업 등 **다단계 처리**에 활용됩니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ac7c9cd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableSequence\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# 컴포넌트 정의\n",
        "prompt = PromptTemplate.from_template(\"'{text}'를 영어로 번역해주세요. 번역된 문장만을 출력해주세요.\")\n",
        "translator = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3, max_tokens=100)\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# RunnableSequence 생성 - 함수 사용 \n",
        "translation_chain = RunnableSequence(\n",
        "    first=prompt,\n",
        "    middle=[translator],   # 리스트로 전달하는 점에 주의\n",
        "    last=output_parser\n",
        ")\n",
        "\n",
        "# RunnableSequence 생성 - 연산자 사용\n",
        "# translation_chain = prompt | translator | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ae5ff8cd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello\n"
          ]
        }
      ],
      "source": [
        "# 동기 실행\n",
        "result = translation_chain.invoke({\"text\": \"안녕하세요\"})\n",
        "print(result)  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04a93db3",
      "metadata": {},
      "source": [
        "**[비동기 처리의 주요 이점]**\n",
        "\n",
        "* 응답성: I/O 작업 중 다른 작업 수행이 가능하여 시스템 전체 응답성 향상\n",
        "\n",
        "* 확장성: 여러 요청을 동시에 처리할 수 있어 처리량 증가\n",
        "\n",
        "* 리소스 활용: CPU와 메모리를 효율적으로 활용하여 성능 최적화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7d2ad50f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# Jupyter에서 비동기 실행을 위한 설정\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# 비동기 실행 함수를 정의\n",
        "async def run():\n",
        "    result = await translation_chain.ainvoke({\"text\": \"감사합니다\"})\n",
        "    print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "01afbeb7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thank you.\n"
          ]
        }
      ],
      "source": [
        "#  비동기 실행 함수를 호출\n",
        "await run()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fb7e8bf",
      "metadata": {},
      "source": [
        "#### 2) **RunnableParallel**\n",
        "\n",
        "- **RunnableParallel**은 여러 컴포넌트를 딕셔너리 형태로 구성해 **동시 실행**합니다\n",
        "\n",
        "- 동일한 입력이 모든 병렬 컴포넌트에 전달되며, 결과는 **키-값 쌍**으로 반환됩니다\n",
        "\n",
        "- **데이터 변환**과 **파이프라인 구성**에 특화되어 있으며, 출력 형식을 다음 단계에 맞게 조정할 수 있습니다"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "625baaa0",
      "metadata": {},
      "source": [
        "`(1) 질문 분석 체인`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1f19926e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "분류 결과: 화학(Chemistry)\n"
          ]
        }
      ],
      "source": [
        "# 질문과 관련된 분야를 찾는 프롬프트\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 출력 파서 정의\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 질문 템플릿 정의\n",
        "question_template = \"\"\"\n",
        "다음 카테고리 중 하나로 입력을 분류하세요:\n",
        "- 화학(Chemistry)\n",
        "- 물리(Physics)\n",
        "- 생물(Biology)\n",
        "\n",
        "# 예시:\n",
        "입력: 사람의 염색체는 모두 몇개가 있나요?\n",
        "답변: 생물(Biology)\n",
        "\n",
        "입력: {question}\n",
        "답변: \"\"\"\n",
        "\n",
        "# 프롬프트 생성\n",
        "question_prompt = ChatPromptTemplate.from_template(question_template)\n",
        "\n",
        "# 체인 구성\n",
        "question_chain = question_prompt | llm | output_parser\n",
        "\n",
        "# 체인 실행\n",
        "result = question_chain.invoke({\"question\": \"탄소의 원자 번호는 무엇인가요?\"})\n",
        "print(f\"분류 결과: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9afa45a8",
      "metadata": {},
      "source": [
        "`(2) 언어 감지 체인`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "f5ce2883",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력: What is the atomic number of carbon?\n",
            "분류 결과: English\n",
            "\n",
            "입력: 탄소의 원자 번호는 무엇인가요?\n",
            "분류 결과: Korean\n",
            "\n",
            "입력: ¿Cuál es el número atómico del carbono?\n",
            "분류 결과: Others\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 질문에 사용된 언어를 구분하는 프롬프트\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 출력 파서 정의\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 언어 분류 템플릿 정의\n",
        "language_template = \"\"\"\n",
        "입력된 텍스트의 언어를 다음 카테고리 중 하나로 분류하세요:\n",
        "- 영어(English)\n",
        "- 한국어(Korean)\n",
        "- 기타(Others)\n",
        "\n",
        "# 예시:\n",
        "입력: How many protons are in a carbon atom?\n",
        "답변: English\n",
        "\n",
        "입력: {question}\n",
        "답변: \"\"\"\n",
        "\n",
        "# 프롬프트 생성\n",
        "language_prompt = ChatPromptTemplate.from_template(language_template)\n",
        "\n",
        "# 체인 구성\n",
        "language_chain = language_prompt | llm | output_parser\n",
        "\n",
        "# 체인 실행 예시\n",
        "examples = [\n",
        "    \"What is the atomic number of carbon?\",\n",
        "    \"탄소의 원자 번호는 무엇인가요?\",\n",
        "    \"¿Cuál es el número atómico del carbono?\"\n",
        "]\n",
        "\n",
        "for example in examples:\n",
        "    result = language_chain.invoke({\"question\": example})\n",
        "    print(f\"입력: {example}\")\n",
        "    print(f\"분류 결과: {result}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c273a56e",
      "metadata": {},
      "source": [
        "`(3) RunnableParallel을 사용한 병렬 실행 체인`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "bf0b47c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "처리 결과:\n",
            "답변: 탄소의 원자 번호는 6입니다. 이는 탄소 원자가 6개의 양성자를 가지고 있음을 의미합니다.\n"
          ]
        }
      ],
      "source": [
        "# 질문과 관련된 분야를 찾아서 질문에 대한 답변을 생성하는 프롬프트\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "\n",
        "# 답변 템플릿 정의\n",
        "answer_template = \"\"\"\n",
        "당신은 {topic} 분야의 전문가입니다. {topic}에 관한 질문에 {language}로 답변해주세요.\n",
        "질문: {question}\n",
        "답변: \"\"\"\n",
        "\n",
        "# 프롬프트 및 체인 구성\n",
        "answer_prompt = ChatPromptTemplate.from_template(answer_template)\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 병렬 처리 체인 구성\n",
        "answer_chain = RunnableParallel({\n",
        "    \"topic\": question_chain,            # 주제 분류 체인\n",
        "    \"language\": language_chain,         # 언어 감지 체인\n",
        "    \"question\": itemgetter(\"question\")  # 원본 질문 추출\n",
        "}) | answer_prompt | llm | output_parser\n",
        "\n",
        "# 체인 실행 예시\n",
        "result = answer_chain.invoke({\n",
        "    \"question\": \"탄소의 원자 번호는 무엇인가요?\"\n",
        "})\n",
        "\n",
        "print(\"처리 결과:\")\n",
        "print(f\"답변: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89ddb27f",
      "metadata": {},
      "source": [
        "#### 3) **RunnablePassthrough**\n",
        "\n",
        "- **RunnablePassthrough**는 입력값을 그대로 전달하여 원본 데이터를 보존합니다\n",
        "\n",
        "- **RunnableParallel**과 함께 사용되어 입력 데이터를 새로운 키로 매핑할 수 있습니다\n",
        "\n",
        "- **투명한 데이터 흐름**으로 파이프라인 디버깅과 구성이 용이합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "11de1469",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'passed': {'query': '탄소의 원자 번호는 6입니다.'}, 'modified': 6}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "import re\n",
        "\n",
        "runnable = RunnableParallel(\n",
        "    passed=RunnablePassthrough(),\n",
        "    modified=lambda x: int(re.search(r'\\d+', x[\"query\"]).group()),\n",
        ")\n",
        "\n",
        "runnable.invoke({\"query\": '탄소의 원자 번호는 6입니다.'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "346a12dc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'passed': '탄소의 원자 번호는 6입니다.', 'modified': 6}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "runnable = RunnableParallel(\n",
        "    passed=RunnablePassthrough(),\n",
        "    modified=lambda x: int(re.search(r'\\d+', x).group()),\n",
        ")\n",
        "\n",
        "runnable.invoke('탄소의 원자 번호는 6입니다.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e888c65",
      "metadata": {
        "id": "2e888c65"
      },
      "source": [
        "#### 4) **RunnableLambda**\n",
        "\n",
        "- **RunnableLambda**는 일반 함수를 Runnable 객체로 변환하는 래퍼 컴포넌트입니다\n",
        "\n",
        "- 체인에 **커스텀 로직**을 쉽게 통합할 수 있어 데이터 전처리, 후처리에 유용합니다\n",
        "\n",
        "- `|` 연산자로 다른 컴포넌트들과 연결해 **복잡한 처리 흐름**을 구성할 수 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f7f8d92f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "# 텍스트에서 숫자를 추출하는 함수\n",
        "def extract_number(query):\n",
        "    return int(re.search(r'\\d+', query).group())\n",
        "\n",
        "# RunnablePassthrough로 입력을 그대로 전달하고, RunnableLambda로 숫자 추출 함수 실행\n",
        "runnable = RunnablePassthrough() | RunnableLambda(extract_number)\n",
        "\n",
        "# 입력 텍스트에서 6을 추출\n",
        "result = runnable.invoke('탄소의 원자 번호는 6입니다.')\n",
        "print(result)  # 출력: 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6704894c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "처리된 응답: ARTIFICIAL INTELLIGENCE (AI) REFERS TO THE SIMULATION OF HUMAN INTELLIGENCE PROCESSES BY MACHINES, PARTICULARLY COMPUTER SYSTEMS, ENABLING THEM TO PERFORM TASKS SUCH AS LEARNING, REASONING, AND PROBLEM-SOLVING.\n",
            "응답 길이: 210\n"
          ]
        }
      ],
      "source": [
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# 데이터 전처리 함수 정의\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\" 입력 텍스트를 소문자로 변환하고 양쪽 공백을 제거합니다. \"\"\"\n",
        "    return text.strip().lower()\n",
        "\n",
        "# 후처리 함수 정의\n",
        "def postprocess_response(response: str) -> dict:\n",
        "    \"\"\" 응답 텍스트를 대문자로 변환하고 길이를 계산합니다. \"\"\"\n",
        "    response_text = response.content\n",
        "    return {\n",
        "        \"processed_response\": response_text.upper(),\n",
        "        \"length\": len(response_text)\n",
        "    }\n",
        "\n",
        "# 프롬프트 템플릿 생성\n",
        "prompt = ChatPromptTemplate.from_template(\"다음 주제에 대해 영어 한 문장으로 설명해주세요: {topic}\")\n",
        "\n",
        "# 처리 파이프라인 구성\n",
        "chain = (\n",
        "    RunnableLambda(preprocess_text) |  # 입력 전처리\n",
        "    prompt |                           # 프롬프트 포맷팅\n",
        "    llm |                              # LLM 추론\n",
        "    RunnableLambda(postprocess_response)  # 출력 후처리\n",
        ")\n",
        "\n",
        "# 체인 실행\n",
        "result = chain.invoke(\"인공지능\")\n",
        "print(f\"처리된 응답: {result['processed_response']}\")\n",
        "print(f\"응답 길이: {result['length']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "367d46b8",
      "metadata": {},
      "source": [
        "# [실습]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd5b260a",
      "metadata": {},
      "source": [
        "- **다음과 같은 요구사항을 LCEL로 구현합니다**\n",
        "   1. 사용자 입력을 받아 요약하기\n",
        "   2. 요약된 내용을 기반으로 감정 분석하기 (긍정, 부정, 중립)\n",
        "   3. 요약된 문장과 감정 분석 결과를 출력하기 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43e424f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 프롬프트 템플릿 정의\n",
        "summarize_prompt = PromptTemplate.from_template(\n",
        "    \"다음 텍스트를 한 문장으로 요약해주세요: {text}\"\n",
        ")\n",
        "\n",
        "sentiment_prompt = PromptTemplate.from_template(\"\"\"\n",
        "다음 텍스트의 감정을 분석해주세요.\n",
        "\n",
        "텍스트: {summary}\n",
        "\n",
        "규칙:\n",
        "1. 반드시 '긍정', '부정', '중립' 중 하나의 단어로만 답변하세요\n",
        "2. 다른 설명이나 부가 정보는 포함하지 마세요\n",
        "\n",
        "답변:\n",
        "\"\"\")\n",
        "\n",
        "# 문자열 출력 파서\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 체인 구성\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# 요약 체인\n",
        "summarize_chain = summarize_prompt | model \n",
        "\n",
        "# 감정 분석 체인\n",
        "sentiment_chain = sentiment_prompt | model | output_parser\n",
        "\n",
        "# 전체 체인\n",
        "chain = (\n",
        "    summarize_chain \n",
        "    | RunnableParallel(\n",
        "        summary=lambda x: x.content,\n",
        "        sentiment=lambda x: sentiment_chain.invoke({\"summary\": x.content}),\n",
        "    )\n",
        ")\n",
        "\n",
        "# 사용 예시\n",
        "text = \"오늘 시험에서 만점을 받았다. 정말 기쁘고 노력한 보람이 있었다!\"\n",
        "result = chain.invoke({\"text\": text})\n",
        "print(f\"요약: {result['summary']}\")\n",
        "print(f\"감정 분석: {result['sentiment']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "modu-01-YKUqNpBc-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
