{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LangChain의 개념과 주요 컴포넌트 이해\n",
    "\n",
    "### **학습 목표:** LangChain의 기본 아키텍처와 주요 컴포넌트를 학습한다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain이란 \n",
    "\n",
    "핵심 내용:\n",
    "- **LangChain**은 LLM 기반 애플리케이션 개발을 위한 프레임워크\n",
    "\n",
    "- **Chain**은 작업을 순차적으로 실행하는 파이프라인 구조를 제공\n",
    "\n",
    "- **Agent**는 자율적 의사결정이 가능한 실행 단위\n",
    "\n",
    "결론:\n",
    "- LangChain은 Chain과 Agent라는 두 가지 핵심 기능을 통해 LLM 애플리케이션 개발을 효율적으로 지원\n",
    "\n",
    "\n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=\"https://python.langchain.com/svg/langchain_stack_112024_dark.svg\" \n",
    "            alt=\"langchain_stack\" \n",
    "            width=\"600\" \n",
    "            style=\"border: 0;\">\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 컴포넌트 \n",
    "\n",
    "핵심 내용:\n",
    "- LangChain **주요 컴포넌트**: LLM/ChatModel, Prompt, Memory, Tool, Document Loader, Text Splitter, Embedding, Vectorstore\n",
    "\n",
    "- **언어 처리 기능**은 LLM/ChatModel이 중심이 되며, Prompt와 Memory로 대화를 관리\n",
    "\n",
    "- **문서 처리와 검색**은 Document Loader, Text Splitter, Embedding, Vectorstore가 담당\n",
    "\n",
    "- **모듈성**이 핵심 특징으로, 독립적인 컴포넌트들을 조합해 RAG와 같은 복잡한 시스템을 구현 가능 \n",
    "\n",
    "결론:\n",
    "- Tool을 통한 확장성과 컴포넌트의 재사용성으로 다양한 LLM 애플리케이션 개발이 가능\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경 변수 로드\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 모델 (Models)\n",
    "- LLM, ChatModel 등으로 구분\n",
    "- OpenAI, Anthropic, Google 등 다양한 모델을 지원\n",
    "- 텍스트 생성, 대화, 요약 등의 작업을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 어떻게 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 10, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-2c25b05f-710b-4a7b-9f51-ad4397c2d242-0', usage_metadata={'input_tokens': 10, 'output_tokens': 11, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "response = model.invoke(\"안녕하세요!\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  안녕하세요! 어떻게 도와드릴까요?\n",
      "메타데이터:  {'token_usage': {'completion_tokens': 11, 'prompt_tokens': 10, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "# 답변 출력\n",
    "print(\"답변: \", response.content)\n",
    "print(\"메타데이터: \", response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 메시지 (Messages)\n",
    "- Chat Model에서 사용할 수 있는 통합된 메시지 형식을 제공\n",
    "- 각 모델 제공자의 특정 메시지 형식을 신경 쓰지 않고도 다양한 채팅 모델을 활용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1. HumanMessage`\n",
    "- 사용자 역할에 해당\n",
    "- 사용자의 입력을 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  \"Glory\"는 한국어로 \"영광\"이라고 번역됩니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 사용자 메시지 생성\n",
    "human_message = HumanMessage(content=\"Glory를 한국어로 번역해주세요.\")\n",
    "\n",
    "# 번역 요청\n",
    "response = model.invoke([human_message])\n",
    "\n",
    "# 답변 출력\n",
    "print(\"답변: \", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"Glory\"를 한국어로 번역하면 \"영광\"입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 17, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None}, id='run-ed033a33-fa1c-4e3d-ae0a-ae07d9f247f9-0', usage_metadata={'input_tokens': 17, 'output_tokens': 18, 'total_tokens': 35, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자열을 입력하면, 자동으로 HumanMessage로 변환하여 요청\n",
    "model.invoke(\"Glory를 한국어로 번역해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2. AIMessage`\n",
    "- AI 모델의 응답을 표현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"Glory\"는 한국어로 \"영광\"이라고 번역됩니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 17, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None}, id='run-4f281006-4a43-4281-ab16-12280f9b00bd-0', usage_metadata={'input_tokens': 17, 'output_tokens': 18, 'total_tokens': 35, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AI 모델의 응답 객체를 출력 \n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Glory\"는 한국어로 \"영광\"이라고 번역됩니다.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 응답 텍스트 부분을 출력\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 17,\n",
       " 'output_tokens': 18,\n",
       " 'total_tokens': 35,\n",
       " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       " 'output_token_details': {'audio': 0, 'reasoning': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 사용량 출력\n",
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3. SystemMessage`\n",
    "- 시스템 역할에 해당\n",
    "- AI 모델의 동작과 제약사항을 정의하는데 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='당신은 영어를 한국어로 번역하는 AI 어시스턴트입니다.', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage \n",
    "\n",
    "# 시스템 메시지 생성\n",
    "system_msg = SystemMessage(content=\"당신은 영어를 한국어로 번역하는 AI 어시스턴트입니다.\")\n",
    "\n",
    "system_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  영광\n"
     ]
    }
   ],
   "source": [
    "# 번역 요청\n",
    "human_message = HumanMessage(content=\"Glory\")\n",
    "\n",
    "messages = [system_msg, human_message]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# 답변 출력\n",
    "print(\"답변: \", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 프롬프트 템플릿 (Prompt Template)\n",
    "- 프롬프트 템플릿을 통해 일관된 입력 형식을 제공\n",
    "    1. 사용자의 입력과 파라미터를 언어 모델이 이해할 수 있는 형태로 변환하는 도구\n",
    "    2. 언어 모델에게 전달할 지시문을 만드는 틀\n",
    "- 변수를 포함한 동적 프롬프트 생성이 가능\n",
    "    1. 모든 템플릿은 딕셔너리 형태의 입력을 받아서 처리\n",
    "    2. 출력은 PromptValue 형태로 반환되며, 이는 문자열이나 메시지 리스트로 변환 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1. 문자열 프롬프트 템플릿 (String PromptTemplate)`\n",
    "- 가장 기본적인 형태\n",
    "- 단일 문자열을 형식화하는데 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='고양이에 대한 이야기를 해줘')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 템플릿 생성\n",
    "template = PromptTemplate.from_template(\"{주제}에 대한 이야기를 해줘\")\n",
    "\n",
    "# 템플릿 사용\n",
    "prompt = template.invoke({\"주제\": \"고양이\"})\n",
    "\n",
    "# 출력\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2. 채팅 프롬프트 템플릿 (ChatPromptTemplate)`\n",
    "- 여러 메시지를 포함하는 대화형 템플릿을 만들 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='당신은 도움이 되는 비서입니다', additional_kwargs={}, response_metadata={}), HumanMessage(content='인공지능에 대해 설명해주세요', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 채팅 템플릿 생성\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 도움이 되는 비서입니다\"),\n",
    "    (\"user\", \"{subject}에 대해 설명해주세요\")\n",
    "])\n",
    "\n",
    "# 템플릿 사용\n",
    "prompt = template.invoke({\"subject\": \"인공지능\"})\n",
    "\n",
    "# 출력\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3. 메시지 플레이스홀더 (MessagesPlaceholder)`\n",
    "- 기존 메시지 목록을 템플릿의 특정 위치에 삽입할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='당신은 도움이 되는 비서입니다', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='안녕하세요!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 메시지 플레이스홀더가 있는 템플릿\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 도움이 되는 비서입니다\"),\n",
    "    MessagesPlaceholder(\"chat_history\")\n",
    "])\n",
    "\n",
    "# 템플릿 사용\n",
    "prompt = template.invoke({\n",
    "    \"chat_history\": [HumanMessage(content=\"안녕하세요!\")]\n",
    "})\n",
    "\n",
    "# 출력\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 출력 파서 (Output Parser)\n",
    "1. **역할과 기능**\n",
    "    - 모델의 텍스트 출력을 구조화된 데이터로 변환\n",
    "    - 채팅 모델과 LLM의 출력을 정규화\n",
    "    - 다운스트림 작업을 위한 데이터 형식 변환\n",
    "\n",
    "2. **사용 시 고려사항**\n",
    "    - OpenAI function calling과 같은 기능이 있는 경우, 해당 기능을 우선 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울은 대한민국의 수도이자 가장 큰 도시로, 다양한 특징을 가지고 있습니다. 아래는 서울의 주요 특징들입니다:\n",
      "\n",
      "1. **역사와 현대의 조화**: 서울은 오랜 역사와 전통을 가진 도시로, 경복궁, 창덕궁과 같은 고궁과 북촌 한옥마을 같은 전통 건축물이 현대적인 고층 건물과 어우러져 있습니다.\n",
      "\n",
      "2. **문화 중심지**: 서울은 다양한 문화예술 활동이 활발한 도시입니다. 국립극장, 서울예술의전당, 갤러리 등에서 공연과 전시가 열리며, 다양한 축제와 행사도 자주 개최됩니다.\n",
      "\n",
      "3. **교통 편리성**: 서울은 지하철, 버스 등 대중교통이 매우 발달해 있어 이동이 편리합니다. 서울지하철은 세계에서 가장 긴 지하철 노선 중 하나입니다.\n",
      "\n",
      "4. **IT와 첨단 산업**: 서울은 IT 산업과 스타트업의 중심지로 부상하고 있으며, 삼성, LG와 같은 대기업의 본사가 위치해 있습니다. 이로 인해 혁신적인 기술과 서비스가 많이 발전하고 있습니다.\n",
      "\n",
      "5. **음식 문화**: 서울은 다양한 한식과 외국 요리를 즐길 수 있는 음식의 도시입니다. 김치, 불고기, 비빔밥, 그리고 길거리 음식 등 다양한 먹거리를 경험할 수 있습니다.\n",
      "\n",
      "6. **자연과 공원**: 서울에는 북한산, 남산, 한강공원 등 자연을 즐길 수 있는 공간이 많아 도심 속에서 여유를 찾을 수 있습니다.\n",
      "\n",
      "7. **관광 명소**: N서울타워, 명동, 인사동, 동대문 디자인 플라자(DDP) 등 많은 관광 명소가 있어 국내외 관광객들이 많이 방문합니다.\n",
      "\n",
      "8. **사회적 다양성**: 서울은 다양한 인종과 문화가 공존하는 도시로, 외국인 거주자와 다양한 문화적 배경을 가진 사람들이 함께 살아가고 있습니다.\n",
      "\n",
      "서울은 이러한 특징들 덕분에 매력적인 도시로 많은 사람들에게 사랑받고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 기본적인 문자열 파서 사용\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 프롬프트 템플릿 설정\n",
    "prompt = PromptTemplate.from_template(\"도시 {city}의 특징을 알려주세요\")\n",
    "\n",
    "# 모델 정의\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "# 체인 구성\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# 체인 실행\n",
    "result = chain.invoke({\"city\": \"서울\"})\n",
    "\n",
    "# 결과 출력\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 메모리 (Memory)\n",
    "- 대화 기록을 저장하고 관리\n",
    "- 컨텍스트 유지를 위한 다양한 메모리 타입을 제공\n",
    "- 대화 요약, 버퍼링 등의 기능을 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# 메모리 기반 히스토리 구현\n",
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "    \n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        self.messages.extend(messages)\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n",
    "\n",
    "# 세션 저장소\n",
    "store = {}\n",
    "\n",
    "# 세션 ID로 히스토리 가져오기\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='1 + 2는 3입니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 31, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-45d00bd5-d447-44f2-917d-6b8a9ae2559e-0' usage_metadata={'input_tokens': 31, 'output_tokens': 10, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# 채팅 모델과 프롬프트 설정\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 {subject}에 능숙한 비서입니다\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "# 히스토리 관리 추가\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# 체인 실행\n",
    "response = chain_with_history.invoke(\n",
    "    {\"subject\": \"수학\", \"question\": \"1+2는 얼마인가요?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user1\"}}\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='1+2는 얼마인가요?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='1 + 2는 3입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 31, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-45d00bd5-d447-44f2-917d-6b8a9ae2559e-0', usage_metadata={'input_tokens': 31, 'output_tokens': 10, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 세션 ID로 히스토리 가져오기\n",
    "get_session_history(\"user1\").messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='3에 2를 곱하면 6입니다. (3 × 2 = 6)' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 62, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-c644ed9a-7043-4a4b-ac36-7b4c4b015578-0' usage_metadata={'input_tokens': 62, 'output_tokens': 22, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# 히스토리 이용해서 대화 진행\n",
    "response = chain_with_history.invoke(\n",
    "    {\"subject\": \"수학\", \"question\": \"여기에 숫자 2를 곱하면 얼마인가요?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user1\"}}\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='1+2는 얼마인가요?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='1 + 2는 3입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 31, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-45d00bd5-d447-44f2-917d-6b8a9ae2559e-0', usage_metadata={'input_tokens': 31, 'output_tokens': 10, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='여기에 숫자 2를 곱하면 얼마인가요?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='3에 2를 곱하면 6입니다. (3 × 2 = 6)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 62, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-c644ed9a-7043-4a4b-ac36-7b4c4b015578-0', usage_metadata={'input_tokens': 62, 'output_tokens': 22, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 세션 ID로 히스토리 가져오기\n",
    "get_session_history(\"user1\").messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습 프로젝트]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 새로운 채팅 세션(\"student1\")을 만들고, 수학 문제를 해결하는 대화를 2회 이상 진행해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='1 + 2는 3입니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 31, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None} id='run-78c755bb-d92a-43eb-8fb4-1decc0404091-0' usage_metadata={'input_tokens': 31, 'output_tokens': 10, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# 체인 실행\n",
    "response = chain_with_history.invoke(\n",
    "    {\"subject\": \"수학\", \"question\": \"1+2는 얼마인가요?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"student1\"}}\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='3에 2를 곱하면 6입니다. (3 × 2 = 6)' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 62, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None} id='run-c4500112-9b0e-4d98-834d-ce96b0343f9b-0' usage_metadata={'input_tokens': 62, 'output_tokens': 22, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# 체인 실행\n",
    "response = chain_with_history.invoke(\n",
    "    {\"subject\": \"수학\", \"question\": \"여기에 숫자 2를 곱하면 얼마인가요?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"student1\"}}\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='1+2는 얼마인가요?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='1 + 2는 3입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 31, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None}, id='run-78c755bb-d92a-43eb-8fb4-1decc0404091-0', usage_metadata={'input_tokens': 31, 'output_tokens': 10, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='여기에 숫자 2를 곱하면 얼마인가요?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='3에 2를 곱하면 6입니다. (3 × 2 = 6)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 62, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None}, id='run-c4500112-9b0e-4d98-834d-ce96b0343f9b-0', usage_metadata={'input_tokens': 62, 'output_tokens': 22, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 세션 ID로 히스토리 가져오기\n",
    "get_session_history(\"student1\").messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 다음 조건을 만족하는 새로운 ChatPromptTemplate을 만드세요.\n",
    "   - 시스템 메시지: \"당신은 친절한 과학 선생님입니다\"\n",
    "   - 대화 기록을 포함\n",
    "   - 사용자 질문을 받을 수 있는 형식\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='당신은 친절한 과학 선생님입니다', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='전자의 반대는 무엇인가요?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# 새로운 채팅 템플릿 생성\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 친절한 과학 선생님입니다\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 템플릿 \n",
    "prompt.invoke({\"history\": [], \"question\": \"전자의 반대는 무엇인가요?\"}).messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. StrOutputParser를 사용하여 다음을 구현해보세요.\n",
    "   - 앞의 프롬프트 템플릿을 사용하여 체인 구성 및 실행\n",
    "   - \"섭씨온도와 화씨온도 관계\"를 설명해 달라고 요청하는 프롬프트 작성 \n",
    "   - 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "섭씨온도(°C)와 화씨온도(°F) 사이의 관계는 다음과 같은 공식으로 표현할 수 있습니다:\n",
      "\n",
      "1. 섭씨에서 화씨로 변환할 때:\n",
      "   \\[\n",
      "   °F = (°C \\times \\frac{9}{5}) + 32\n",
      "   \\]\n",
      "\n",
      "2. 화씨에서 섭씨로 변환할 때:\n",
      "   \\[\n",
      "   °C = (°F - 32) \\times \\frac{5}{9}\n",
      "   \\]\n",
      "\n",
      "이 관계를 통해 두 온도 단위 간의 변환을 쉽게 할 수 있습니다.\n",
      "\n",
      "### 예시:\n",
      "- 섭씨 0도는 화씨 32도입니다.\n",
      "- 섭씨 100도는 화씨 212도입니다.\n",
      "\n",
      "온도 단위는 서로 다른 기준을 가지고 있으며, 일반적으로 섭씨는 물의 동결점과 끓는점을 기준으로 하고, 화씨는 미국에서 더 많이 사용됩니다. 섭씨는 물의 상태 변화와 관련된 과학적 연구에 주로 사용되는 반면, 화씨는 일상적인 날씨 보고에서 자주 사용됩니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 체인 구성\n",
    "sicence_chain = prompt | ChatOpenAI(model='gpt-4o-mini') | StrOutputParser()\n",
    "\n",
    "# 히스토리 관리 추가\n",
    "sicence_chain_with_history = RunnableWithMessageHistory(\n",
    "    sicence_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# 체인 실행\n",
    "response = sicence_chain_with_history.invoke(\n",
    "    {\"question\": \"섭씨온도와 화씨온도 관계에 대해 설명해주세요\"},\n",
    "    config={\"configurable\": {\"session_id\": \"student1\"}}\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faq-bot-dxmBTzxG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
