{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1d2631b5",
      "metadata": {},
      "source": [
        "#  Gradio 챗봇 구현 (간단한 QA 애플리케이션)\n",
        "\n",
        "### **학습 목표:** LangChain의 LCEL을 활용하여 Gradio 기반의 AI 챗봇을 설계한다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bfd9328",
      "metadata": {
        "id": "8bfd9328"
      },
      "source": [
        "##  환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "15ac3ddb",
      "metadata": {
        "id": "15ac3ddb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "129b8eee",
      "metadata": {
        "id": "129b8eee"
      },
      "source": [
        "## Simple QA Chain  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3722db67",
      "metadata": {
        "id": "3722db67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "파이썬에서 리스트를 정렬하는 방법은 여러 가지가 있습니다. 가장 일반적인 방법은 `sort()` 메서드와 `sorted()` 함수를 사용하는 것입니다.\n",
            "\n",
            "1. **`sort()` 메서드**:\n",
            "   - 리스트 객체에 직접 적용되며, 리스트를 제자리에서 정렬합니다. 즉, 원래 리스트가 변경됩니다.\n",
            "   - 기본적으로 오름차순으로 정렬됩니다.\n",
            "\n",
            "   ```python\n",
            "   my_list = [3, 1, 4, 1, 5, 9]\n",
            "   my_list.sort()\n",
            "   print(my_list)  # 출력: [1, 1, 3, 4, 5, 9]\n",
            "   ```\n",
            "\n",
            "   - 내림차순으로 정렬하려면 `reverse=True` 인자를 사용할 수 있습니다.\n",
            "\n",
            "   ```python\n",
            "   my_list.sort(reverse=True)\n",
            "   print(my_list)  # 출력: [9, 5, 4, 3, 1, 1]\n",
            "   ```\n",
            "\n",
            "2. **`sorted()` 함수**:\n",
            "   - 리스트를 인자로 받아 새로운 정렬된 리스트를 반환합니다. 원래 리스트는 변경되지 않습니다.\n",
            "\n",
            "   ```python\n",
            "   my_list = [3, 1, 4, 1, 5, 9]\n",
            "   sorted_list = sorted(my_list)\n",
            "   print(sorted_list)  # 출력: [1, 1, 3, 4, 5, 9]\n",
            "   print(my_list)      # 원래 리스트는 변경되지 않음\n",
            "   ```\n",
            "\n",
            "   - `sorted()` 함수도 `reverse=True` 인자를 사용하여 내림차순으로 정렬할 수 있습니다.\n",
            "\n",
            "   ```python\n",
            "   sorted_list_desc = sorted(my_list, reverse=True)\n",
            "   print(sorted_list_desc)  # 출력: [9, 5, 4, 3, 1, 1]\n",
            "   ```\n",
            "\n",
            "3. **사용자 정의 정렬 기준**:\n",
            "   - `key` 인자를 사용하여 정렬 기준을 지정할 수 있습니다. 예를 들어, 문자열의 길이에 따라 정렬할 수 있습니다.\n",
            "\n",
            "   ```python\n",
            "   words = [\"banana\", \"pie\", \"Washington\", \"book\"]\n",
            "   words.sort(key=len)\n",
            "   print(words)  # 출력: ['pie', 'book', 'banana', 'Washington']\n",
            "   ```\n",
            "\n",
            "이와 같이 파이썬에서는 리스트를 정렬하는 다양한 방법을 제공하므로, 필요에 따라 적절한 방법을 선택하여 사용할 수 있습니다.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 프롬프트 템플릿 정의\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 파이썬(Python) 코드 작성을 도와주는 AI 어시스턴트입니다.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# LLM 모델 정의\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\", \n",
        "    temperature=0.3, \n",
        "    )\n",
        "\n",
        "# 프롬프트 템플릿 + LLM 모델 + 출력파서를 연결하여 체인 생성\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "# 체인 실행\n",
        "response = chain.invoke({\n",
        "    \"user_input\": \"파이썬에서 리스트를 정렬하는 방법은 무엇인가요?\"\n",
        "})\n",
        "\n",
        "# AI의 응답 텍스트를 출력 \n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9e87eef4",
      "metadata": {
        "id": "9e87eef4"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "파이썬에서 리스트를 정렬하는 방법은 여러 가지가 있습니다. 가장 일반적인 방법은 `sort()` 메서드와 `sorted()` 함수를 사용하는 것입니다.\n",
              "\n",
              "1. **`sort()` 메서드**:\n",
              "   - 리스트 객체에 직접 호출하여 리스트를 정렬합니다.\n",
              "   - 원본 리스트를 수정하며, 반환값은 `None`입니다.\n",
              "\n",
              "```python\n",
              "my_list = [5, 2, 9, 1, 5, 6]\n",
              "my_list.sort()  # 오름차순 정렬\n",
              "print(my_list)  # [1, 2, 5, 5, 6, 9]\n",
              "\n",
              "my_list.sort(reverse=True)  # 내림차순 정렬\n",
              "print(my_list)  # [9, 6, 5, 5, 2, 1]\n",
              "```\n",
              "\n",
              "2. **`sorted()` 함수**:\n",
              "   - 리스트를 인자로 받아 정렬된 새로운 리스트를 반환합니다.\n",
              "   - 원본 리스트는 변경되지 않습니다.\n",
              "\n",
              "```python\n",
              "my_list = [5, 2, 9, 1, 5, 6]\n",
              "sorted_list = sorted(my_list)  # 오름차순 정렬\n",
              "print(sorted_list)  # [1, 2, 5, 5, 6, 9]\n",
              "print(my_list)  # 원본 리스트는 변경되지 않음: [5, 2, 9, 1, 5, 6]\n",
              "\n",
              "sorted_list_desc = sorted(my_list, reverse=True)  # 내림차순 정렬\n",
              "print(sorted_list_desc)  # [9, 6, 5, 5, 2, 1]\n",
              "```\n",
              "\n",
              "3. **사용자 정의 정렬**:\n",
              "   - `key` 매개변수를 사용하여 정렬 기준을 지정할 수 있습니다.\n",
              "\n",
              "```python\n",
              "my_list = ['apple', 'banana', 'cherry', 'date']\n",
              "my_list.sort(key=len)  # 문자열의 길이에 따라 정렬\n",
              "print(my_list)  # ['apple', 'date', 'banana', 'cherry']\n",
              "```\n",
              "\n",
              "이와 같이 `sort()` 메서드와 `sorted()` 함수를 사용하여 리스트를 쉽게 정렬할 수 있습니다."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 마크다운 출력\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d47a59f9",
      "metadata": {},
      "source": [
        "## Gradio ChatInterface  \n",
        "- 설치: pip install gradio --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e17f810a",
      "metadata": {},
      "source": [
        "### 1) 기본 구조"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "65443d80",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/wiggler/Library/Caches/pypoetry/virtualenvs/faq-bot-dxmBTzxG-py3.12/lib/python3.12/site-packages/gradio/components/chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# 챗봇 함수 정의\n",
        "def chat_function(message, history):\n",
        "    return \"응답 메시지\"\n",
        "\n",
        "# 챗봇 인터페이스 생성\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_function,  # 실행할 함수\n",
        "    analytics_enabled=False,  # 사용 정보 제공 여부\n",
        ")\n",
        "\n",
        "# 챗봇 인터페이스 실행\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b6df357c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "# 인터페이스 종료\n",
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d070bc3e",
      "metadata": {},
      "source": [
        "### 2) 간단한 예제: Echo 챗봇"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8df39e39",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def echo_bot(message, history):\n",
        "    return f\"당신이 입력한 메시지: {message}\"\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=echo_bot,\n",
        "    title=\"Echo 챗봇\",\n",
        "    description=\"입력한 메시지를 그대로 되돌려주는 챗봇입니다.\",\n",
        "    analytics_enabled=False,  \n",
        ")\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "eea11f99",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20c3c244",
      "metadata": {},
      "source": [
        "### 3) 스트리밍 응답"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "18d36896",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 스트리밍 챗봇 함수 정의\n",
        "import time\n",
        "\n",
        "def streaming_bot(message, history):\n",
        "    response = f\"처리 중인 메시지: {message}\"\n",
        "    for i in range(len(response)):\n",
        "        time.sleep(0.1)          # 0.1초 대기\n",
        "        yield response[:i+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4f38cc55",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 스트리밍 챗봇 인터페이스 생성\n",
        "demo = gr.ChatInterface(\n",
        "    fn=streaming_bot,\n",
        "    title=\"스트리밍 챗봇\",\n",
        "    description=\"입력한 메시지를 한 글자씩 처리하는 챗봇입니다.\",\n",
        "    analytics_enabled=False,  \n",
        ")\n",
        "\n",
        "# 스트리밍 챗봇 인터페이스 실행\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9ffdd862",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5129fcd",
      "metadata": {},
      "source": [
        "### 4) 추가 입력 컴포넌트\n",
        "- 최대 응답 길이 등 기타 설정을 위한 추가 입력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6069ca",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/wiggler/Library/Caches/pypoetry/virtualenvs/faq-bot-dxmBTzxG-py3.12/lib/python3.12/site-packages/gradio/components/chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "# 프롬프트 템플릿 정의\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 파이썬(Python) 코드 작성을 도와주는 AI 어시스턴트입니다.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "\n",
        "# 챗봇 함수 정의\n",
        "def chat_function(message, history, model, temperature):\n",
        "\n",
        "    if model == \"gpt-4o-mini\":\n",
        "        model = ChatOpenAI(model=model, temperature=temperature)\n",
        "    elif model == \"gemini-1.5-flash\":\n",
        "        model = ChatGoogleGenerativeAI(model=model, temperature=temperature)\n",
        "\n",
        "    chain = prompt | model | StrOutputParser()\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"user_input\": message\n",
        "    })\n",
        "    return response\n",
        "\n",
        "# 챗봇 인터페이스 생성\n",
        "with gr.Blocks() as demo:\n",
        "    model_selector = gr.Dropdown([\"gpt-4o-mini\", \"gemini-1.5-flash\"], label=\"모델 선택\")\n",
        "    slider = gr.Slider(0.0, 1.0, label=\"Temperature\", value=0.3, step=0.1, render=False)   \n",
        "\n",
        "    gr.ChatInterface(\n",
        "        fn=chat_function, \n",
        "        additional_inputs=[model_selector, slider],\n",
        "        analytics_enabled=False,  \n",
        "    )\n",
        "\n",
        "# 챗봇 인터페이스 실행\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ff42f636",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e05c70ea",
      "metadata": {},
      "source": [
        "### 5) 예시 질문 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c44c455e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/wiggler/Library/Caches/pypoetry/virtualenvs/faq-bot-dxmBTzxG-py3.12/lib/python3.12/site-packages/gradio/components/chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 스트리밍 챗봇 인터페이스 생성\n",
        "demo = gr.ChatInterface(\n",
        "    fn=streaming_bot,\n",
        "    title=\"스트리밍 챗봇\",\n",
        "    description=\"입력한 메시지를 한 글자씩 처리하는 챗봇입니다.\",\n",
        "    analytics_enabled=False,  \n",
        "    examples=[\n",
        "        \"파이썬 코드를 작성하는 방법을 알려주세요\",\n",
        "        \"파이썬에서 리스트를 정렬하는 방법은 무엇인가요?\",\n",
        "    ]    \n",
        ")\n",
        "\n",
        "# 스트리밍 챗봇 인터페이스 실행\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "b16cb24f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "demo.close()    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0309e370",
      "metadata": {},
      "source": [
        "### 6) 멀티모달 기능\n",
        "- `multimodal=True` 옵션\n",
        "- 이미지나 파일을 처리할 수 있는 멀티모달 챗봇 구현\n",
        "\n",
        "- message 파라미터:\n",
        "    ```python\n",
        "    {\n",
        "        \"text\": \"user input\", \n",
        "        \"files\": [\n",
        "            \"updated_file_1_path.ext\",\n",
        "            \"updated_file_2_path.ext\", \n",
        "            ...\n",
        "        ]\n",
        "    }\n",
        "    ```\n",
        "- history 파라미터:\n",
        "    ```python\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": (\"cat1.png\")},\n",
        "        {\"role\": \"user\", \"content\": (\"cat2.png\")},\n",
        "        {\"role\": \"user\", \"content\": \"What's the difference between these two images?\"},\n",
        "    ]\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "709da9f9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/wiggler/Library/Caches/pypoetry/virtualenvs/faq-bot-dxmBTzxG-py3.12/lib/python3.12/site-packages/gradio/components/chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "History: []\n",
            "Filepath list: ['/private/var/folders/vp/t7xb2kg161q5m2ylkq9jn7k00000gn/T/gradio/67a38971cfcf1a3506f50814fffecf2995ea4c7b7a24ecce42e15603078ff1cd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg']\n",
            "History: [[('/private/var/folders/vp/t7xb2kg161q5m2ylkq9jn7k00000gn/T/gradio/67a38971cfcf1a3506f50814fffecf2995ea4c7b7a24ecce42e15603078ff1cd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg',), '이 이미지는 푸른 하늘 아래 잔디밭을 가로지르는 나무 산책로를 보여줍니다. 하늘은 파란색이며 가늘고 깃털 같은 흰 구름이 흩어져 있습니다. 잔디는 생생한 녹색이며 산책로 양쪽에 무성하게 자랍니다. 산책로는 나무 판자로 만들어져 있으며 부드럽게 잔디밭으로 사라집니다. 배경에는 나무와 관목이 있습니다. 전반적인 분위기는 평화롭고 고요합니다.']]\n",
            "Filepath list: ['/private/var/folders/vp/t7xb2kg161q5m2ylkq9jn7k00000gn/T/gradio/67a38971cfcf1a3506f50814fffecf2995ea4c7b7a24ecce42e15603078ff1cd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg']\n",
            "History: [[('/private/var/folders/vp/t7xb2kg161q5m2ylkq9jn7k00000gn/T/gradio/67a38971cfcf1a3506f50814fffecf2995ea4c7b7a24ecce42e15603078ff1cd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg',), '이 이미지는 푸른 하늘 아래 잔디밭을 가로지르는 나무 산책로를 보여줍니다. 하늘은 파란색이며 가늘고 깃털 같은 흰 구름이 흩어져 있습니다. 잔디는 생생한 녹색이며 산책로 양쪽에 무성하게 자랍니다. 산책로는 나무 판자로 만들어져 있으며 부드럽게 잔디밭으로 사라집니다. 배경에는 나무와 관목이 있습니다. 전반적인 분위기는 평화롭고 고요합니다.'], ['사람은 몇 명인가요?', '이 이미지에는 사람이 없습니다. 나무판자 산책로가 있는 풀이 무성한 풍경이 있습니다.']]\n",
            "Filepath list: ['/private/var/folders/vp/t7xb2kg161q5m2ylkq9jn7k00000gn/T/gradio/67a38971cfcf1a3506f50814fffecf2995ea4c7b7a24ecce42e15603078ff1cd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg', '/private/var/folders/vp/t7xb2kg161q5m2ylkq9jn7k00000gn/T/gradio/2dd4115007cfe39f67177d44d6eb83a606f1d25fa5885d1ff0da591a9a9416b6/KakaoTalk_Photo_2024-12-20-15-33-16.png']\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import base64\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def convert_to_url(image_path):\n",
        "    \"\"\"이미지를 URL 형식으로 변환\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        # 이미지를 base64로 인코딩\n",
        "        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "        return f\"data:image/jpeg;base64,{encoded_string}\"\n",
        "\n",
        "def multimodal_bot(message, history):\n",
        "\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
        "    \n",
        "    if isinstance(message, dict):\n",
        "        # 텍스트와 파일 추출\n",
        "        text = message.get(\"text\", \"\")\n",
        "        \n",
        "        # 히스토리와 현재 메시지에서 모든 파일 경로 추출\n",
        "        filepath_list = []\n",
        "        \n",
        "        # 히스토리에서 이미지 파일 추출\n",
        "        print(\"History:\", history)  # 디버깅용\n",
        "        for exchange in history:\n",
        "            user_message = exchange[0]\n",
        "            if isinstance(user_message, tuple):  # 이미지 메시지 확인\n",
        "                filepath_list.append(user_message[0])\n",
        "        \n",
        "        # 현재 메시지의 파일들도 추가\n",
        "        files = message.get(\"files\", [])\n",
        "        filepath_list.extend(files)\n",
        "        \n",
        "        print(\"Filepath list:\", filepath_list)  # 디버깅용\n",
        "        \n",
        "        if filepath_list:\n",
        "            # 모든 이미지 처리\n",
        "            image_urls = []\n",
        "            for file_path in filepath_list:\n",
        "                try:\n",
        "                    image_url = convert_to_url(file_path)\n",
        "                    image_urls.append({\"type\": \"image_url\", \"image_url\": image_url})\n",
        "                except Exception as e:\n",
        "                    print(f\"이미지 처리 중 오류 발생: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            if not image_urls:\n",
        "                return \"이미지 처리 중 오류가 발생했습니다.\"\n",
        "            \n",
        "            # 메시지 구성\n",
        "            content = [\n",
        "                {\"type\": \"text\", \"text\": text if text else \"이 이미지들에 대해 설명해주세요.\"},\n",
        "                *image_urls\n",
        "            ]\n",
        "            \n",
        "            try:\n",
        "                # API 호출\n",
        "                response = model.invoke([\n",
        "                    HumanMessage(content=content)\n",
        "                ])\n",
        "                return response.content\n",
        "            except Exception as e:\n",
        "                return f\"모델 응답 생성 중 오류가 발생했습니다: {str(e)}\"\n",
        "        \n",
        "        return text if text else \"이미지를 업로드해주세요.\"\n",
        "    \n",
        "    return \"텍스트나 이미지를 입력해주세요.\"\n",
        "\n",
        "# Gradio 인터페이스 설정\n",
        "demo = gr.ChatInterface(\n",
        "    fn=multimodal_bot,\n",
        "    multimodal=True,\n",
        "    title=\"멀티모달 챗봇\",\n",
        "    description=\"텍스트와 이미지를 함께 처리할 수 있는 챗봇입니다. 이전 대화의 이미지들도 함께 고려합니다.\",\n",
        "    analytics_enabled=False,  \n",
        "    textbox=gr.MultimodalTextbox(placeholder=\"텍스트를 입력하거나 이미지를 업로드해주세요.\", file_count=\"multiple\", file_types=[\"image\"]),\n",
        ")\n",
        "\n",
        "# 인터페이스 실행\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "e8086535",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ae3e9b8",
      "metadata": {},
      "source": [
        "### 7) PDF 뷰어\n",
        "- 설치: pip install gradio_pdf 또는 poetry add gradio_pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "f63e62dc",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/wiggler/Library/Caches/pypoetry/virtualenvs/faq-bot-dxmBTzxG-py3.12/lib/python3.12/site-packages/gradio/components/chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7861\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gradio_pdf import PDF\n",
        "\n",
        "def answer_invoke(message, history):   \n",
        "    return message\n",
        "\n",
        "with gr.Blocks(\n",
        "    analytics_enabled=False,  \n",
        ") as demo:\n",
        "    with gr.Row():\n",
        "        # API Key Section\n",
        "        api_key_input = gr.Textbox(\n",
        "            label=\"Enter OpenAI API Key\",\n",
        "            type=\"password\",\n",
        "            placeholder=\"sk-...\"\n",
        "        )\n",
        "        \n",
        "    with gr.Row():\n",
        "        # PDF Upload and Chat Interface\n",
        "        with gr.Column(scale=2):\n",
        "            pdf_file = PDF(\n",
        "                label=\"Upload PDF File\",\n",
        "                height=600,  # PDF 뷰어 높이 설정\n",
        "            )\n",
        "        with gr.Column(scale=1):\n",
        "            chatbot = gr.ChatInterface(\n",
        "                fn=answer_invoke,\n",
        "                title=\"PDF-based Chatbot\",\n",
        "                description=\"Upload a PDF file and ask questions about its contents.\",\n",
        "            )\n",
        "\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "db19ce00",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7861\n"
          ]
        }
      ],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7369f37",
      "metadata": {},
      "source": [
        "## Memory 추가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c3d508b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/wiggler/Library/Caches/pypoetry/virtualenvs/modu-01-YKUqNpBc-py3.12/lib/python3.12/site-packages/gradio/components/chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "History: []\n",
            "History: [['안녕하세요. 제 이름은 스티브입니다. ', '안녕하세요, 스티브님! 만나서 반갑습니다. 어떻게 도와드릴까요? 파이썬 관련 질문이나 다른 궁금한 점이 있으시면 말씀해 주세요!']]\n"
          ]
        }
      ],
      "source": [
        "# chat_history 플레이스홀더를 사용\n",
        "import gradio as gr\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# 메시지 플레이스홀더가 있는 프롬프트 템플릿 정의\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 파이썬(Python) 코드 작성을 도와주는 AI 어시스턴트입니다.\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"system\", \"이전 대화 내용을 참고하여 질문에 대해서 친절하게 답변합니다.\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# 프롬프트 템플릿 + LLM 모델 + 출력파서를 연결하여 체인 생성\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "\n",
        "# 사용자 메시지를 처리하고 AI 응답을 생성하는 함수 (chat_history 사용)\n",
        "def answer_invoke(message, history):\n",
        "\n",
        "    print(\"History:\", history)  # 디버깅용\n",
        "\n",
        "    history_messages = []\n",
        "    for human_msg, ai_msg in history:\n",
        "        history_messages.extend([\n",
        "            HumanMessage(content=human_msg),\n",
        "            AIMessage(content=ai_msg)\n",
        "        ])\n",
        "    \n",
        "    history_messages.append(HumanMessage(content=message))\n",
        "    response = chain.invoke({\n",
        "        \"chat_history\": history_messages,\n",
        "        \"user_input\": message\n",
        "    })\n",
        "    return response\n",
        "    \n",
        "\n",
        "# Gradio ChatInterface 객체 생성\n",
        "demo = gr.ChatInterface(\n",
        "    fn=answer_invoke,         # 메시지 처리 함수\n",
        "    title=\"파이썬 코드 어시스턴트\", # 채팅 인터페이스의 제목\n",
        "    )\n",
        "\n",
        "# Gradio 인터페이스 실행\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "576dbb2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "# Gradio 인터페이스 종료\n",
        "demo.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "367d46b8",
      "metadata": {},
      "source": [
        "# [실습 프로젝트]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd5b260a",
      "metadata": {},
      "source": [
        "- **다음과 같은 요구사항을 Gradio ChatInterface로 구현합니다**\n",
        "\n",
        "- 주제: 맞춤형 여행 일정 계획 어시스턴트\n",
        "- 기능: \n",
        "   - OpenAI Chat Completion API와 LangChain을 활용하여 사용자의 선호도에 맞는 여행 일정을 생성\n",
        "   - LCEL을 사용하여 단계별 프롬프트 체인 구성 (사용자 입력 분석 -> 일정 생성 -> 세부 계획 수립)\n",
        "   - 채팅 히스토리 사용하여 답변 생성\n",
        "   - Gradio 인터페이스를 통해 사용자와 대화형으로 상호작용\n",
        "\n",
        "- 주요 포인트:\n",
        "\n",
        "   1. **모델 매개변수 최적화**\n",
        "      - temperature=0.7: 적당한 창의성을 유지하면서 일관된 응답 생성\n",
        "      - top_p=0.9: 높은 확률의 토큰만 선택하여 응답의 품질 향상\n",
        "      - presence_penalty와 frequency_penalty: 반복적인 응답을 줄이고 다양한 제안 생성\n",
        "\n",
        "   2. **시스템 프롬프트 설계**\n",
        "      - 여행 플래너로서의 역할과 응답 가이드라인을 명확히 정의\n",
        "      - 구체적인 정보를 포함하도록 지시\n",
        "      - 한국어 응답 명시\n",
        "\n",
        "   3. **메모리 관리**\n",
        "      - Gradio 또는 LangChain 메모리 기능을 사용하여 대화 컨텍스트 유지\n",
        "      - 이전 대화 내용을 바탕으로 연속성 있는 응답 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e20357",
      "metadata": {},
      "source": [
        "[예시 답안]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "469bcb5a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "# Gradio 인터페이스 종료\n",
        "demo.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5f6a91ab",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/wiggler/Library/Caches/pypoetry/virtualenvs/modu-01-YKUqNpBc-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/Users/wiggler/Library/Caches/pypoetry/virtualenvs/modu-01-YKUqNpBc-py3.12/lib/python3.12/site-packages/gradio/components/chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "의도 분석: 의도: 추천요청  \n",
            "상세: 겨울철에 친구들과 함께 여행하기 좋은 장소에 대한 추천을 요청하고 있음.\n",
            "의도 분석: [의도 분석]\n",
            "의도: 일정계획  \n",
            "상세: 온천 여행지 중 한 곳을 선택하여 구체적인 일정 계획을 제안해달라는 요청.\n",
            "\n",
            "[답변]\n",
            "온천 여행지로 **일본의 하코네**를 추천합니다. 하코네는 아름다운 자연 경관과 다양한 온천 시설이 있어 휴식을 취하기에 최적의 장소입니다. 아래는 2박 3일 일정 제안입니다.\n",
            "\n",
            "### 1일차: 하코네 도착 및 온천 체험\n",
            "- **오전**: 도쿄에서 출발 (기차 이용)\n",
            "- **오후**: 하코네 도착 후 숙소 체크인 (온천 료칸 추천)\n",
            "  - 예시 숙소: 하코네 유모토 온천 지역의 '하코네 소운칸'\n",
            "- **저녁**: 료칸에서 제공하는 전통 일본식 저녁식사(카이세키) 즐기기\n",
            "- **밤**: 료칸 내 온천에서 편안한 시간 보내기\n",
            "\n",
            "### 2일차: 하코네 관광 및 온천 탐방\n",
            "- **아침**: 료칸에서 아침식사 후 체크아웃\n",
            "- **오전**: 하코네 신사 방문\n",
            "  - 아름다운 경치와 함께 신사를 둘러보고, 사진 촬영\n",
            "- **점심**: 근처의 로컬 식당에서 '하코네 우동' 또는 '소바' 즐기기\n",
            "- **오후**:\n",
            "  - 하코네 고라 공원 방문, 케이블카 타고 고라산 전망대 감상\n",
            "  - 하코네 미술관 방문하여 예술작품 관람\n",
            "- **저녁**: 다른 온천 시설에서 저녁 식사 후 온천 체험 (예시: '하코네 텐유노야도')\n",
            "- **밤**: 다시 료칸으로 돌아가거나, 다른 숙소 예약 가능\n",
            "\n",
            "### 3일차: 자연 탐방 및 귀환\n",
            "- **아침**: 료칸에서 아침식사 후 체크아웃\n",
            "- **오전**:\n",
            "  - 아시 호수 주변 산책 및 보트 투어 (호수 위에서 후지산 전망)\n",
            "- **점심**: 아시 호수 근처 카페에서 점심 식사\n",
            "- **오후**:\n",
            "  - 자유시간 또는 추가적으로 주변 명소 탐방 (예: 오와쿠다니 계곡)\n",
            "- **저녁**: 도쿄로 돌아가는 기차 탑승\n",
            "\n",
            "이 일정을 통해 하코네의 아름다움을 만끽하며 친구들과 함께 잊지 못할 추억을 만들 수 있을 것입니다!\n",
            "상세 계획: 온천 여행지 중 일본의 **하코네**를 선정하여 상세 일정을 제안합니다. 하코네는 아름다운 자연과 다양한 온천이 있는 인기 있는 여행지입니다.\n",
            "\n",
            "### 1. 여행 개요\n",
            "- **목적지**: 일본 하코네\n",
            "- **기간**: 3일 2박 (금요일~일요일)\n",
            "- **예산 범위**: 1인당 약 30만 원 (항공료 제외, 숙소, 식사, 교통비 포함)\n",
            "\n",
            "### 2. 일자별 세부 일정\n",
            "\n",
            "#### **1일차 (금요일)**\n",
            "- **09:00**: 인천국제공항 출발 (비행기 이동)\n",
            "- **11:30**: 나리타/하네다 공항 도착\n",
            "- **12:30**: 공항에서 하코네로 이동 (약 2시간 소요, 전철 이용)\n",
            "  - *비용*: 약 2만 원\n",
            "- **14:30**: 하코네 도착 후 점심식사 (현지 레스토랑)\n",
            "  - *비용*: 약 1만 원\n",
            "- **15:30**: 하코네 유람선 탑승 및 아시노코 호수 관광 (약 1시간)\n",
            "  - *비용*: 약 1만 원\n",
            "- **17:00**: 숙소 체크인 (온천 리조트)\n",
            "  - 추천 숙소: 하코네 유모토 온천 호텔\n",
            "- **18:00**: 저녁식사 (숙소 내 식당 또는 근처 음식점)\n",
            "  - *비용*: 약 2만 원\n",
            "- **20:00**: 온천욕 및 휴식\n",
            "\n",
            "#### **2일차 (토요일)**\n",
            "- **08:00**: 조식 (숙소 내 식당)\n",
            "- **09:00**: 하코네 신사 방문 (약 1시간 소요)\n",
            "- **10:30**: 하코네 미술관 탐방 (약 1시간 소요)\n",
            "  - *비용*: 약 1만 원\n",
            "- **12:00**: 점심식사 (현지 맛집 탐방)\n",
            "  - *비용*: 약 1만 원\n",
            "- **13:30**: 오와쿠다니 계곡 탐방 및 흑계란 시식 (약 2시간 소요)\n",
            "- **16:00**: 다시 숙소로 돌아가 온천욕 및 휴식\n",
            "- **19:00**: 저녁식사 (근처 이자카야 체험)\n",
            "  - *비용*: 약 2만 원\n",
            "- **21:00**: 자유시간 또는 온천욕\n",
            "\n",
            "#### **3일차 (일요일)**\n",
            "- **08:00**: 조식 후 체크아웃\n",
            "- **09:30**: 하코네 고라 공원 방문 및 케이블카 체험(약 2시간 소요)\n",
            "  - *비용*: 약 2만 원\n",
            "- **11:30**: 점심식사 후 기념품 쇼핑\n",
            "   - *비용*: 약 1만 원\n",
            "- **13:00** : 하코네에서 나리타/하네다 공항으로 이동 \n",
            "   - *이동 시간*: 약 2시간 \n",
            "   - *비용*: 약 2만 원 \n",
            "- **15:30** : 공항 도착 및 면세 쇼핑 \n",
            "- **18:00** : 인천국제공항 도착\n",
            "\n",
            "### 총 예상 비용:\n",
            "| 항목         | 비용(원) |\n",
            "|--------------|----------|\n",
            "| 교통비       |    8만원 |\n",
            "| 숙박(2박)    |   10만원 |\n",
            "| 식사(6끼)     |    12만원 |\n",
            "| 관광지 입장료 |    ~4만원 |\n",
            "| 총합         |   ~34만원 |\n",
            "\n",
            "### 주요 관광지 정보:\n",
            "1. **하코네 신사:** 아름다운 경치와 역사적인 신사가 있는 장소.\n",
            "2. **하코네 미술관:** 일본 전통 예술과 현대 예술이 어우러진 공간.\n",
            "3. **오와쿠다니 계곡:** 활화산 지역으로 유명하며, 흑계란을 맛볼 수 있습니다.\n",
            "4. **아시노코 호수:** 유람선에서 바라보는 후지산의 멋진 경치가 매력적입니다.\n",
            "\n",
            "### 숙박 및 식사 계획:\n",
            "- 추천 숙소는 하코네 유모토 온천 호텔로, 객실에서 바로 온천을 즐길 수 있습니다.\n",
            "- 식사는 현지 맛집과 숙소 내 레스토랑을 이용할 예정이며, 다양한 일본 요리를 경험할 수 있습니다.\n",
            "\n",
            "### 현지 정보:\n",
            "- 일본의 겨울 날씨는 춥고 눈이 올 수 있으므로 따뜻한 옷을 준비하세요.\n",
            "- 대중교통은 편리하게 연결되어 있으며, 영어 표기가 되어 있어 이동에 큰 어려움이 없습니다.\n",
            "  \n",
            "이번 하코네 여행이 친구들과 함께하는 즐거운 추억으로 남길 바랍니다!\n",
            "의도 분석: 의도: 기타  \n",
            "상세: 상대방에게 반가움을 표현하고 대화를 시작하려는 의도입니다.\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    presence_penalty=0.3,\n",
        "    frequency_penalty=0.3\n",
        ")\n",
        "\n",
        "\n",
        "# 프롬프트 템플릿 정의\n",
        "intent_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"다음 메시지의 의도를 파악하여 아래 형식으로 출력하세요:\\n의도: [일정계획/정보요청/추천요청/기타]\\n상세: (구체적인 의도 설명)\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{message}\")\n",
        "])\n",
        "\n",
        "\n",
        "plan_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"다음 여행 요청에 대해 아래 항목을 포함한 상세 계획을 생성하세요:\n",
        "1. 여행 개요\n",
        "   - 목적지, 기간, 예산 범위\n",
        "2. 일자별 세부 일정\n",
        "   - 시간대별 방문지\n",
        "   - 이동 수단과 소요 시간\n",
        "   - 예상 비용\n",
        "3. 주요 관광지 정보\n",
        "4. 숙박 및 식사 계획\n",
        "5. 현지 정보\"\"\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{message}\")\n",
        "])\n",
        "\n",
        "\n",
        "def analyze_intent(message: str, history_messages: list) -> str:\n",
        "    response = llm.invoke(\n",
        "        intent_template.format_messages(\n",
        "            message=message,\n",
        "            chat_history=history_messages\n",
        "        )\n",
        "    )\n",
        "    return response.content\n",
        "\n",
        "\n",
        "def create_detailed_plan(message: str, history_messages: list) -> str:\n",
        "    response = llm.invoke(\n",
        "        plan_template.format_messages(\n",
        "            message=message,\n",
        "            chat_history=history_messages\n",
        "        )\n",
        "    )\n",
        "    return response.content\n",
        "\n",
        "\n",
        "def process_message(message: str, history_messages: list) -> str:\n",
        "    # 의도 분석\n",
        "    intent = analyze_intent(message, history_messages)\n",
        "    print(f\"의도 분석: {intent}\")\n",
        "   \n",
        "    # 의도에 따른 응답 생성\n",
        "    if \"일정계획\" in intent:\n",
        "        plan = create_detailed_plan(message, history_messages)\n",
        "        print(f\"상세 계획: {plan}\")\n",
        "        return f\"[의도 분석]\\n{intent}\\n\\n[상세 계획]\\n{plan}\"\n",
        "    else:\n",
        "        general_template = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"여행 관련 질문에 대해 이전 대화를 고려하여 구체적으로 답변해주세요.\"),\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\"human\", \"{message}\")\n",
        "        ])\n",
        "        response = llm.invoke(\n",
        "            general_template.format_messages(\n",
        "                message=message,\n",
        "                chat_history=history_messages\n",
        "            )\n",
        "        )\n",
        "        return f\"[의도 분석]\\n{intent}\\n\\n[답변]\\n{response.content}\"\n",
        "\n",
        "\n",
        "def answer_invoke(message: str, history: List) -> str:\n",
        "    history_messages = []\n",
        "    for human_msg, ai_msg in history:\n",
        "        history_messages.extend([\n",
        "            HumanMessage(content=human_msg),\n",
        "            AIMessage(content=ai_msg)\n",
        "        ])\n",
        "    return process_message(message, history_messages[-5:])\n",
        "\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=answer_invoke,\n",
        "    title=\"맞춤형 여행 일정 계획 어시스턴트\",\n",
        "    description=\"여행 일정 계획과 관련된 질문을 해주세요. 이전 대화 내용을 기억하여 답변합니다.\"\n",
        ")\n",
        "\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "58d46146",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "# Gradio 인터페이스 종료\n",
        "demo.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95889696",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "modu-01-YKUqNpBc-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
